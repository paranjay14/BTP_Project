['options.ckptDir:batchKmeans', 'options.maxDepth:1', 'options.cnnLR:0.001', 'options.mlpLR:0.001', 'options.cnnEpochs:100', 'options.mlpEpochs:60', 'options.cnnOut:32', 'options.mlpFC1:1024', 'options.mlpFC2:32']
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
len(trainInputDict["data"]):  49000 ,  len(valInputDict["data"]):  1000 ,  len(testInputDict["data"]):  10000
nodeId:  1 , imgTensorShape :  torch.Size([49000, 3, 32, 32])
nodeId: 1 ,  parentId: 0 ,  level: 0 ,  lchildId: -1 ,  rchildId: -1 ,  isLeaf: False ,  leafClass: -1 ,  numClasses: 10 ,  numData: 49000
Running nodeId:  1
0 Train Loss: 181.696 | Train Acc: 37.198 | Val Loss: 1.506 | Val Accuracy: 48.600
10 Train Loss: 96.041 | Train Acc: 67.088 | Val Loss: 1.137 | Val Accuracy: 61.200
20 Train Loss: 74.727 | Train Acc: 74.935 | Val Loss: 1.164 | Val Accuracy: 60.700
30 Train Loss: 60.400 | Train Acc: 79.849 | Val Loss: 1.213 | Val Accuracy: 61.400
40 Train Loss: 46.864 | Train Acc: 85.339 | Val Loss: 1.325 | Val Accuracy: 60.400
Epoch     6: reducing learning rate of group 0 to 2.0000e-04.
50 Train Loss: 37.310 | Train Acc: 88.976 | Val Loss: 1.447 | Val Accuracy: 60.300
60 Train Loss: 29.855 | Train Acc: 92.927 | Val Loss: 1.475 | Val Accuracy: 60.300
Epoch     8: reducing learning rate of group 0 to 4.0000e-05.
70 Train Loss: 28.042 | Train Acc: 93.549 | Val Loss: 1.506 | Val Accuracy: 60.100
80 Train Loss: 26.328 | Train Acc: 94.359 | Val Loss: 1.516 | Val Accuracy: 59.900
Epoch    10: reducing learning rate of group 0 to 8.0000e-06.
90 Train Loss: 25.903 | Train Acc: 94.480 | Val Loss: 1.526 | Val Accuracy: 59.800
CNN trained successfully...
image_next_flat.shape :  torch.Size([49000, 25088])
Kmeans trained successfully...
printing expected split from k means
{8: 0, 5: 0, 2: 0, 4: 0, 9: 0, 7: 0, 0: 0, 3: 0, 1: 1, 6: 0}
Printing final_dict items...
{0: 0, 8: 0, 4: 0, 2: 0, 7: 0, 9: 1, 6: 1, 3: 1, 5: 1, 1: 1}
Image Statistics before MLP : L R :  24500 24500
expectedMlpLabels.shape :  torch.Size([49000])
0 Loss: 106.004 | Acc: 73.653
1 Loss: 89.245 | Acc: 78.998
2 Loss: 80.392 | Acc: 81.478
3 Loss: 73.466 | Acc: 83.335
4 Loss: 68.250 | Acc: 84.669
5 Loss: 62.543 | Acc: 86.159
6 Loss: 58.210 | Acc: 87.141
7 Loss: 54.891 | Acc: 88.112
8 Loss: 49.801 | Acc: 89.186
9 Loss: 46.209 | Acc: 90.167
10 Loss: 34.386 | Acc: 92.808
11 Loss: 30.569 | Acc: 93.653
12 Loss: 27.773 | Acc: 94.286
13 Loss: 25.623 | Acc: 94.722
14 Loss: 23.660 | Acc: 95.147
15 Loss: 22.067 | Acc: 95.504
16 Loss: 19.911 | Acc: 96.065
17 Loss: 19.252 | Acc: 96.247
18 Loss: 18.335 | Acc: 96.498
19 Loss: 17.035 | Acc: 96.686
20 Loss: 12.464 | Acc: 97.710
21 Loss: 10.596 | Acc: 98.076
22 Loss: 9.983 | Acc: 98.220
23 Loss: 9.228 | Acc: 98.259
24 Loss: 9.121 | Acc: 98.337
25 Loss: 8.478 | Acc: 98.451
26 Loss: 8.085 | Acc: 98.496
27 Loss: 7.678 | Acc: 98.594
28 Loss: 7.406 | Acc: 98.657
29 Loss: 7.368 | Acc: 98.673
30 Loss: 5.842 | Acc: 98.984
31 Loss: 5.754 | Acc: 98.969
32 Loss: 5.553 | Acc: 99.018
33 Loss: 4.975 | Acc: 99.139
34 Loss: 4.839 | Acc: 99.184
35 Loss: 4.698 | Acc: 99.141
36 Loss: 4.367 | Acc: 99.239
37 Loss: 4.813 | Acc: 99.171
38 Loss: 4.574 | Acc: 99.210
39 Loss: 4.537 | Acc: 99.204
40 Loss: 3.973 | Acc: 99.333
41 Loss: 3.796 | Acc: 99.318
42 Loss: 3.771 | Acc: 99.312
43 Loss: 3.878 | Acc: 99.335
44 Loss: 4.011 | Acc: 99.300
45 Loss: 3.546 | Acc: 99.388
46 Loss: 3.841 | Acc: 99.369
47 Loss: 3.423 | Acc: 99.435
48 Loss: 3.424 | Acc: 99.437
49 Loss: 3.705 | Acc: 99.341
50 Loss: 3.313 | Acc: 99.431
51 Loss: 3.140 | Acc: 99.490
52 Loss: 3.217 | Acc: 99.441
53 Loss: 3.254 | Acc: 99.465
54 Loss: 3.247 | Acc: 99.418
55 Loss: 3.276 | Acc: 99.445
56 Loss: 3.440 | Acc: 99.439
57 Loss: 3.165 | Acc: 99.437
58 Loss: 2.980 | Acc: 99.504
59 Loss: 2.939 | Acc: 99.498
MLP trained successfully...
